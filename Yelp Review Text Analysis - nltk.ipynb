{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Working with Yelp Review Text using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read in review data\n",
    "## To get the full dataset, use the below: \n",
    "rvws = pd.read_csv(\"data\\yelp_academic_dataset_review.csv\")\n",
    "\n",
    "## Below csv was created previously using the first 5,000 reviews. Once this is a working model we can expand it to a fuller dataset.\n",
    "# reviews = pd.read_csv(\"data\\yelp_academic_dataset_review.csv\")\n",
    "# rvws = reviews.iloc[:5000,:]\n",
    "# rvws.to_csv(\"yelp_dataset_5k_review.csv\")\n",
    "#rvws = pd.read_csv(\"data\\yelp_dataset_5k_review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## To make things easier when trying to analyze the text, let's just look at restaurants. For that, we'll have to join the review\n",
    "## and business data. I'm also pulling the city and state so we can split this up by city, based on the assumption that\n",
    "## language will vary depending on location.\n",
    "\n",
    "headers = ['categories','business_id','city','state']\n",
    "business = pd.read_csv(\"data\\yelp_academic_dataset_business.csv\", usecols = headers)\n",
    "reviews_full = pd.merge(rvws, business, on = 'business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Now that we have the data joined with the business information, we can narrow the dataset to just restaurants so that\n",
    "## the language we are looking for is more consistent. \n",
    "## (e.g. a good doctor review will have differnt language than a good restaurant review)\n",
    "\n",
    "cat = reviews_full['categories']\n",
    "rest = []\n",
    "\n",
    "for x in range(len(cat)):\n",
    "    if \"Restaurants\" in cat[x]:\n",
    "        rest.append(1)\n",
    "    elif \"Food\" in cat[x]:\n",
    "        rest.append(1)\n",
    "    else:\n",
    "        rest.append(0)\n",
    "\n",
    "reviews_full['restaurant'] = rest\n",
    "\n",
    "## This will modify our reviews DataFrame to only include business that fall into the \"restaurants\" category,\n",
    "## Then reset the index.\n",
    "\n",
    "reviews_full = reviews_full[reviews_full['restaurant'] == 1]\n",
    "reviews_full = reviews_full.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Las Vegas          402889\n",
       "Phoenix            156592\n",
       "Scottsdale          85399\n",
       "Charlotte           67038\n",
       "Pittsburgh          50538\n",
       "Tempe               47012\n",
       "Henderson           32641\n",
       "Chandler            30578\n",
       "Madison             29697\n",
       "Mesa                27842\n",
       "Montr√©al            23631\n",
       "Gilbert             19281\n",
       "Glendale            16960\n",
       "Edinburgh           13999\n",
       "Montreal            12746\n",
       "Peoria               8209\n",
       "Champaign            6772\n",
       "North Las Vegas      6458\n",
       "Surprise             5283\n",
       "Goodyear             4891\n",
       "Avondale             3560\n",
       "Queen Creek          3042\n",
       "Cave Creek           2994\n",
       "Urbana               2826\n",
       "Matthews             2587\n",
       "Middleton            1978\n",
       "Karlsruhe            1710\n",
       "Fort Mill            1656\n",
       "Waterloo             1480\n",
       "Concord              1333\n",
       "dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## In addition to narrowing it down to one city to work with. We will revisit the impact of location after doing as much\n",
    "## as we can with just the text for all reviews in one city.\n",
    "\n",
    "city = reviews_full['city']\n",
    "city.value_counts()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>votes.cool</th>\n",
       "      <th>business_id</th>\n",
       "      <th>votes.funny</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>votes.useful</th>\n",
       "      <th>categories</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>restaurant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V0y4fqp-4pSRfsz0FmsjPA</td>\n",
       "      <td>r9Ow00PF8y7fv32Xtg4Gzg</td>\n",
       "      <td>So....You say you want authentico?? My family ...</td>\n",
       "      <td>8</td>\n",
       "      <td>cvhsdSDaWPjLVnTThQkx8g</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2008-04-23</td>\n",
       "      <td>review</td>\n",
       "      <td>12</td>\n",
       "      <td>[u'Mexican', u'Restaurants']</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Glendale</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L87hrFVD-53NV8xRvoWfUw</td>\n",
       "      <td>DagBfm4lxPUYqQ78haUJ9Q</td>\n",
       "      <td>I'm giving El Conquistador 4 stars for the foo...</td>\n",
       "      <td>0</td>\n",
       "      <td>cvhsdSDaWPjLVnTThQkx8g</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2008-12-14</td>\n",
       "      <td>review</td>\n",
       "      <td>1</td>\n",
       "      <td>[u'Mexican', u'Restaurants']</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Glendale</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ogwv5L0BkHxxojLQc_ENIg</td>\n",
       "      <td>lf_nd6GVq_Rx19hLZENNOw</td>\n",
       "      <td>My husband and I moved to Phoenix from San Die...</td>\n",
       "      <td>0</td>\n",
       "      <td>cvhsdSDaWPjLVnTThQkx8g</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>review</td>\n",
       "      <td>2</td>\n",
       "      <td>[u'Mexican', u'Restaurants']</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Glendale</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gMU7u0IvLF8xF8zDPS7lvg</td>\n",
       "      <td>59o6fNgnwmC5_hhQDX5XgA</td>\n",
       "      <td>In a nutshell:  the food is delicious, the own...</td>\n",
       "      <td>0</td>\n",
       "      <td>cvhsdSDaWPjLVnTThQkx8g</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2009-05-10</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>[u'Mexican', u'Restaurants']</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Glendale</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VBw-kUZsxbIMwDGHJ7dl1w</td>\n",
       "      <td>JG81y1xgwJLPMIPBm33ZFA</td>\n",
       "      <td>El Conquistador is the best Mexican restaurant...</td>\n",
       "      <td>0</td>\n",
       "      <td>cvhsdSDaWPjLVnTThQkx8g</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009-08-14</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>[u'Mexican', u'Restaurants']</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Glendale</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id               review_id  \\\n",
       "0  V0y4fqp-4pSRfsz0FmsjPA  r9Ow00PF8y7fv32Xtg4Gzg   \n",
       "1  L87hrFVD-53NV8xRvoWfUw  DagBfm4lxPUYqQ78haUJ9Q   \n",
       "2  Ogwv5L0BkHxxojLQc_ENIg  lf_nd6GVq_Rx19hLZENNOw   \n",
       "3  gMU7u0IvLF8xF8zDPS7lvg  59o6fNgnwmC5_hhQDX5XgA   \n",
       "4  VBw-kUZsxbIMwDGHJ7dl1w  JG81y1xgwJLPMIPBm33ZFA   \n",
       "\n",
       "                                                text  votes.cool  \\\n",
       "0  So....You say you want authentico?? My family ...           8   \n",
       "1  I'm giving El Conquistador 4 stars for the foo...           0   \n",
       "2  My husband and I moved to Phoenix from San Die...           0   \n",
       "3  In a nutshell:  the food is delicious, the own...           0   \n",
       "4  El Conquistador is the best Mexican restaurant...           0   \n",
       "\n",
       "              business_id  votes.funny  stars        date    type  \\\n",
       "0  cvhsdSDaWPjLVnTThQkx8g            7      5  2008-04-23  review   \n",
       "1  cvhsdSDaWPjLVnTThQkx8g            0      3  2008-12-14  review   \n",
       "2  cvhsdSDaWPjLVnTThQkx8g            0      5  2009-04-06  review   \n",
       "3  cvhsdSDaWPjLVnTThQkx8g            0      4  2009-05-10  review   \n",
       "4  cvhsdSDaWPjLVnTThQkx8g            0      5  2009-08-14  review   \n",
       "\n",
       "   votes.useful                    categories state      city  restaurant  \n",
       "0            12  [u'Mexican', u'Restaurants']    AZ  Glendale           1  \n",
       "1             1  [u'Mexican', u'Restaurants']    AZ  Glendale           1  \n",
       "2             2  [u'Mexican', u'Restaurants']    AZ  Glendale           1  \n",
       "3             0  [u'Mexican', u'Restaurants']    AZ  Glendale           1  \n",
       "4             0  [u'Mexican', u'Restaurants']    AZ  Glendale           1  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = reviews_full[reviews_full['city'] == 'Avondale']\n",
    "reviews = reviews.reset_index(drop=True)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for each star so we can get a general sense of distribution:\n",
      "5    5604\n",
      "4    4995\n",
      "3    2548\n",
      "1    2053\n",
      "2    1760\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x75636a58>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs8AAAFxCAYAAACMWkUvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x48a009b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = reviews['text']\n",
    "stars = reviews['stars']\n",
    "\n",
    "print \"Value counts for each star so we can get a general sense of distribution:\"\n",
    "print stars.value_counts()\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(12,6))\n",
    "stars.hist(color='red', alpha=0.5, bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>votes.cool</th>\n",
       "      <th>business_id</th>\n",
       "      <th>votes.funny</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>votes.useful</th>\n",
       "      <th>categories</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V0y4fqp-4pSRfsz0FmsjPA</td>\n",
       "      <td>r9Ow00PF8y7fv32Xtg4Gzg</td>\n",
       "      <td>So....You say you want authentico?? My family ...</td>\n",
       "      <td>8</td>\n",
       "      <td>cvhsdSDaWPjLVnTThQkx8g</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2008-04-23</td>\n",
       "      <td>review</td>\n",
       "      <td>12</td>\n",
       "      <td>[u'Mexican', u'Restaurants']</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Glendale</td>\n",
       "      <td>1</td>\n",
       "      <td>1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L87hrFVD-53NV8xRvoWfUw</td>\n",
       "      <td>DagBfm4lxPUYqQ78haUJ9Q</td>\n",
       "      <td>I'm giving El Conquistador 4 stars for the foo...</td>\n",
       "      <td>0</td>\n",
       "      <td>cvhsdSDaWPjLVnTThQkx8g</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2008-12-14</td>\n",
       "      <td>review</td>\n",
       "      <td>1</td>\n",
       "      <td>[u'Mexican', u'Restaurants']</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Glendale</td>\n",
       "      <td>1</td>\n",
       "      <td>1292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ogwv5L0BkHxxojLQc_ENIg</td>\n",
       "      <td>lf_nd6GVq_Rx19hLZENNOw</td>\n",
       "      <td>My husband and I moved to Phoenix from San Die...</td>\n",
       "      <td>0</td>\n",
       "      <td>cvhsdSDaWPjLVnTThQkx8g</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009-04-06</td>\n",
       "      <td>review</td>\n",
       "      <td>2</td>\n",
       "      <td>[u'Mexican', u'Restaurants']</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Glendale</td>\n",
       "      <td>1</td>\n",
       "      <td>552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gMU7u0IvLF8xF8zDPS7lvg</td>\n",
       "      <td>59o6fNgnwmC5_hhQDX5XgA</td>\n",
       "      <td>In a nutshell:  the food is delicious, the own...</td>\n",
       "      <td>0</td>\n",
       "      <td>cvhsdSDaWPjLVnTThQkx8g</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2009-05-10</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>[u'Mexican', u'Restaurants']</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Glendale</td>\n",
       "      <td>1</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VBw-kUZsxbIMwDGHJ7dl1w</td>\n",
       "      <td>JG81y1xgwJLPMIPBm33ZFA</td>\n",
       "      <td>El Conquistador is the best Mexican restaurant...</td>\n",
       "      <td>0</td>\n",
       "      <td>cvhsdSDaWPjLVnTThQkx8g</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009-08-14</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>[u'Mexican', u'Restaurants']</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Glendale</td>\n",
       "      <td>1</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id               review_id  \\\n",
       "0  V0y4fqp-4pSRfsz0FmsjPA  r9Ow00PF8y7fv32Xtg4Gzg   \n",
       "1  L87hrFVD-53NV8xRvoWfUw  DagBfm4lxPUYqQ78haUJ9Q   \n",
       "2  Ogwv5L0BkHxxojLQc_ENIg  lf_nd6GVq_Rx19hLZENNOw   \n",
       "3  gMU7u0IvLF8xF8zDPS7lvg  59o6fNgnwmC5_hhQDX5XgA   \n",
       "4  VBw-kUZsxbIMwDGHJ7dl1w  JG81y1xgwJLPMIPBm33ZFA   \n",
       "\n",
       "                                                text  votes.cool  \\\n",
       "0  So....You say you want authentico?? My family ...           8   \n",
       "1  I'm giving El Conquistador 4 stars for the foo...           0   \n",
       "2  My husband and I moved to Phoenix from San Die...           0   \n",
       "3  In a nutshell:  the food is delicious, the own...           0   \n",
       "4  El Conquistador is the best Mexican restaurant...           0   \n",
       "\n",
       "              business_id  votes.funny  stars        date    type  \\\n",
       "0  cvhsdSDaWPjLVnTThQkx8g            7      5  2008-04-23  review   \n",
       "1  cvhsdSDaWPjLVnTThQkx8g            0      3  2008-12-14  review   \n",
       "2  cvhsdSDaWPjLVnTThQkx8g            0      5  2009-04-06  review   \n",
       "3  cvhsdSDaWPjLVnTThQkx8g            0      4  2009-05-10  review   \n",
       "4  cvhsdSDaWPjLVnTThQkx8g            0      5  2009-08-14  review   \n",
       "\n",
       "   votes.useful                    categories state      city  restaurant  \\\n",
       "0            12  [u'Mexican', u'Restaurants']    AZ  Glendale           1   \n",
       "1             1  [u'Mexican', u'Restaurants']    AZ  Glendale           1   \n",
       "2             2  [u'Mexican', u'Restaurants']    AZ  Glendale           1   \n",
       "3             0  [u'Mexican', u'Restaurants']    AZ  Glendale           1   \n",
       "4             0  [u'Mexican', u'Restaurants']    AZ  Glendale           1   \n",
       "\n",
       "   text_length  \n",
       "0         1009  \n",
       "1         1292  \n",
       "2          552  \n",
       "3          498  \n",
       "4          564  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's create some data based on the text so we can run some regressions on characteristics of the text.\n",
    "\n",
    "text_length = []\n",
    "for x in range(len(reviews['text'])):\n",
    "    text_length.append(len(text[x]))\n",
    "\n",
    "reviews['text_length'] = text_length\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Downloads from here: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\n",
    "## Grab a generic words of positive and negative words; we are going to use these to get a sentiment score out of the text.\n",
    "\n",
    "positives = open('data/positive-words.txt').read()\n",
    "negatives = open('data/negative-words.txt').read()\n",
    "pos_word_list = positives.split('\\n')\n",
    "neg_word_list = negatives.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create function to output list of words in given list from a sentence.\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "\n",
    "    review_text = review   \n",
    "    \n",
    "    # 1. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "\n",
    "    # 2. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    # 5. Return a list of words\n",
    "    return(words)\n",
    "\n",
    "def word_count(sentence, word_list):\n",
    "    count = 0\n",
    "    for x in review_to_wordlist(sentence):\n",
    "        if x in word_list:\n",
    "            count += 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## In an initial run of this test, two separate values were created, which counted the positive and negative words in the corpus.\n",
    "## Upon further review, this was deleted and replace with a sentiment score which was 1 for all positive words\n",
    "## and -1 for all negative words\n",
    "\n",
    "#pos_word_count = []\n",
    "#neg_word_count = []\n",
    "#pos_neg_ratio = []\n",
    "#for x in text:\n",
    "#    positive = word_count(x, pos_word_list)\n",
    "#    negative = word_count(x, neg_word_list)\n",
    "#    if (negative > 0):\n",
    "#        ratio = float(positive)/negative\n",
    "#    else:\n",
    "#        ratio = positive\n",
    "#    \n",
    "#    if ((positive + negative) == 0):\n",
    "#        sent = 0\n",
    "#    else:\n",
    "#        sent = ((positive-negative)/(float(positive)+negative))\n",
    "#        \n",
    "#    pos_word_count.append(positive)\n",
    "#    neg_word_count.append(negative)\n",
    "#    pos_neg_ratio.append(ratio)\n",
    "\n",
    "#reviews['pos_word_count'] = pos_word_count\n",
    "#reviews['neg_word_count'] = neg_word_count\n",
    "#reviews['sentiment'] = sentiment\n",
    "#reviews['pos_neg_ratio'] = pos_neg_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring text #0\n",
      "Scoring text #1000\n",
      "Scoring text #2000\n",
      "Scoring text #3000\n",
      "Scoring text #4000\n",
      "Scoring text #5000\n",
      "Scoring text #6000\n",
      "Scoring text #7000\n",
      "Scoring text #8000\n",
      "Scoring text #9000\n",
      "Scoring text #10000\n",
      "Scoring text #11000\n",
      "Scoring text #12000\n",
      "Scoring text #13000\n",
      "Scoring text #14000\n",
      "Scoring text #15000\n",
      "Scoring text #16000\n",
      "Wall time: 4min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Use positive and negative words to give each piece of a text a sentiment score. The sentiment score ranges from\n",
    "## -1 (all negative words) to 1 (all positive words).\n",
    "\n",
    "sentiment = []\n",
    "pos_neg_ratio = []\n",
    "count = 0\n",
    "\n",
    "for x in text:\n",
    "    positive = word_count(x, pos_word_list)\n",
    "    negative = word_count(x, neg_word_list)\n",
    "    if (negative > 0):\n",
    "        ratio = float(positive)/negative\n",
    "    else:\n",
    "        ratio = positive\n",
    "    \n",
    "    if ((positive + negative) == 0):\n",
    "        sent = 1\n",
    "    else:\n",
    "        sent = ((positive-negative)/(float(positive)+negative))+1\n",
    "        \n",
    "    sentiment.append(sent)\n",
    "    pos_neg_ratio.append(ratio)\n",
    "    if count % 1000 == 0:\n",
    "        print \"Scoring text #{}\".format(count)\n",
    "    count += 1\n",
    "\n",
    "reviews['sentiment'] = sentiment\n",
    "reviews['pos_neg_ratio'] = pos_neg_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Features used for initial pass: text length, ratio of positive to negative words, and sentiment.\n",
    "\n",
    "feature_list = ['text_length', 'pos_neg_ratio', 'sentiment']\n",
    "features = reviews[feature_list]\n",
    "stars = reviews[['stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test using Various models. Set up the train test split first, then fit various models and see how they perform.\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samchu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:15: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "C:\\Users\\samchu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:449: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree score: 0.313089622642\n",
      "Random Forest score: 0.318199685535\n",
      "Multinomial Naive Bayes score: 0.394261006289\n",
      "Linear SVC Score: 0.302279874214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samchu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:449: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "## Fit various models\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(features, stars, test_size=0.3, random_state=12)\n",
    "tree = DecisionTreeClassifier()\n",
    "forest = RandomForestClassifier()\n",
    "mnbayes = MultinomialNB()\n",
    "linearSVC = LinearSVC()\n",
    "\n",
    "tree.fit(x_train, y_train)\n",
    "forest.fit(x_train, y_train)\n",
    "mnbayes.fit(x_train, y_train)\n",
    "linearSVC.fit(x_train, y_train)\n",
    "\n",
    "print \"Decision Tree score: {}\".format(tree.score(x_test, y_test))\n",
    "print \"Random Forest score: {}\".format(forest.score(x_test, y_test))\n",
    "print \"Multinomial Naive Bayes score: {}\".format(mnbayes.score(x_test, y_test))\n",
    "print \"Linear SVC Score: {}\".format(linearSVC.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Before trying to move on to a new method, let's make this a binary predictor by\n",
    "## separating reviews into \"good\" (4-5 stars) and \"bad\" (1-3 stars)\n",
    "\n",
    "# Playing with different star thresholds for what we consider a \"good\" or \"bad\" review.\n",
    "star_threshold = 4\n",
    "\n",
    "reviews['good'] = (reviews['stars'] >= star_threshold)\n",
    "good = reviews['good']\n",
    "good = good.map({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree score: 0.665290880503\n",
      "Random Forest score: 0.689465408805\n",
      "Multinomial Naive Bayes score: 0.731328616352\n",
      "Linear SVC Score: 0.658215408805 \n",
      "\n",
      "Decision Tree AUC: 0.666007720206\n",
      "Random Forest AUC: 0.730189074515\n",
      "Multinomial Naive Bayes AUC: 0.777821703267\n",
      "Linear SVC AUC: 0.698933182595\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(features, good, test_size=0.3, random_state=12)\n",
    "tree = DecisionTreeClassifier()\n",
    "forest = RandomForestClassifier()\n",
    "mnbayes = MultinomialNB()\n",
    "linearSVC = LinearSVC()\n",
    "\n",
    "tree.fit(x_train, y_train)\n",
    "forest.fit(x_train, y_train)\n",
    "mnbayes.fit(x_train, y_train)\n",
    "linearSVC.fit(x_train, y_train)\n",
    "\n",
    "print \"Decision Tree score: {}\".format(tree.score(x_test, y_test))\n",
    "print \"Random Forest score: {}\".format(forest.score(x_test, y_test))\n",
    "print \"Multinomial Naive Bayes score: {}\".format(mnbayes.score(x_test, y_test))\n",
    "print \"Linear SVC Score: {} \\n\".format(linearSVC.score(x_test, y_test))\n",
    "\n",
    "print \"Decision Tree AUC: {}\".format(cross_val_score(tree, features, good, cv=3, scoring='roc_auc').mean())\n",
    "print \"Random Forest AUC: {}\".format(cross_val_score(forest, features, good, cv=3, scoring='roc_auc').mean())\n",
    "print \"Multinomial Naive Bayes AUC: {}\".format(cross_val_score(mnbayes, features, good, cv=3, scoring='roc_auc').mean())\n",
    "print \"Linear SVC AUC: {}\".format(cross_val_score(linearSVC, features, good, cv=3, scoring='roc_auc').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##Let's manipulate the data a little more, then vectorize the text and fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words(\"english\"))\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# le = WordNetLemmatizer()\n",
    "# asdf = ['cat','cats','catting','about','pooping','poop','poops','a','pooper','be']\n",
    "# for x in asdf:\n",
    "#     print le.lemmatize(x)\n",
    "# asdf = [le.lemmatize(z) for z in asdf if not z in stop_words]\n",
    "# asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text #1000...\n",
      "Cleaning text #2000...\n",
      "Cleaning text #3000...\n",
      "Cleaning text #4000...\n",
      "Cleaning text #5000...\n",
      "Cleaning text #6000...\n",
      "Cleaning text #7000...\n",
      "Cleaning text #8000...\n",
      "Cleaning text #9000...\n",
      "Cleaning text #10000...\n",
      "Cleaning text #11000...\n",
      "Cleaning text #12000...\n",
      "Cleaning text #13000...\n",
      "Cleaning text #14000...\n",
      "Cleaning text #15000...\n",
      "Cleaning text #16000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samchu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-c1f990b08654>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'\\n## Let\\'s work with the text before vectorizing. We\\'re going to get rid of stop words and non-letters.\\n## We\\'re also going to run the text through a lemmatizer, which will help narrow down the feature set.\\n## This will take some time.\\n\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.stem.wordnet import WordNetLemmatizer\\n\\nstop_words = set(stopwords.words(\"english\"))\\ncount = 0\\ntext_clean = text\\nlemm = WordNetLemmatizer()\\n\\nfor x in range(len(text_clean)):\\n    count +=1\\n    if count % 1000 == 0:\\n        print \"Cleaning text #{}...\".format(count)\\n    w = text[x]\\n    w = re.sub(\"[^a-zA-Z]\",\" \", w)    # gets rid of non-letters and substitutes it with a space\\n    w = word_tokenize(w)    # Tokenizes the sentence, breaking it up into a list of words rather than one long string.\\n    w = [lemm.lemmatize(z) for z in w if not z in stop_words]    # deletes stop words from the text, or words that don\\'t add any meaning\\n    text_clean[x] = w\\n\\n# text_clean = [\\' , \\'.join(z).strip() for z in text]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\samchu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2262\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2263\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2264\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2265\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\samchu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\IPython\\core\\magics\\execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\samchu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\IPython\\core\\magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\samchu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\IPython\\core\\magics\\execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1164\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1166\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1167\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\samchu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;31m# do the setitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m         \u001b[0mcacher_needs_updating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_chained_assignment_possible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m         \u001b[0msetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcacher_needs_updating\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\samchu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36msetitem\u001b[1;34m(key, value)\u001b[0m\n\u001b[0;32m    635\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0msetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_with_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSettingWithCopyError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\samchu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36m_set_with_engine\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_with_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\samchu\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36mvalues\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0marr\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m--> 317\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Let's work with the text before vectorizing. We're going to get rid of stop words and non-letters.\n",
    "## We're also going to run the text through a lemmatizer, which will help narrow down the feature set.\n",
    "## This will take some time.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "count = 0\n",
    "text_clean = text\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "for x in range(len(text_clean)):\n",
    "    count +=1\n",
    "    if count % 1000 == 0:\n",
    "        print \"Cleaning text #{}...\".format(count)\n",
    "    w = text[x]\n",
    "    w = re.sub(\"[^a-zA-Z]\",\" \", w)    # gets rid of non-letters and substitutes it with a space\n",
    "    w = word_tokenize(w)    # Tokenizes the sentence, breaking it up into a list of words rather than one long string.\n",
    "    w = [lemm.lemmatize(z) for z in w if not z in stop_words]    # deletes stop words from the text, or words that don't add any meaning\n",
    "    text_clean[x] = w\n",
    "\n",
    "text_clean = [' , '.join(z).strip() for z in text_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Vectorize the text with sklearn's CountVectorizer.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(decode_error = 'ignore')\n",
    "vect.fit(text_clean)\n",
    "text_dtm = vect.transform(text_clean)\n",
    "text_array = text_dtm.toarray()\n",
    "text_df = pd.DataFrame(text_array, columns = vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree score: 0.363068688671\n",
      "Random Forest score: 0.348795718109\n",
      "Multinomial Naive Bayes score: 0.448706512043\n"
     ]
    }
   ],
   "source": [
    "## Initial test by running the test on the vectorization of the text.\n",
    "\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(text_array, stars, test_size=0.3, random_state=12)\n",
    "tree = DecisionTreeClassifier()\n",
    "forest = RandomForestClassifier()\n",
    "mnbayes = MultinomialNB()\n",
    "\n",
    "tree.fit(x_train, y_train)\n",
    "forest.fit(x_train, y_train)\n",
    "mnbayes.fit(x_train, y_train)\n",
    "linearSVC.fit(x_train, y_train)\n",
    "\n",
    "print \"Decision Tree score: {}\".format(tree.score(x_test, y_test))\n",
    "print \"Random Forest score: {}\".format(forest.score(x_test, y_test))\n",
    "print \"Multinomial Naive Bayes score: {}\".format(mnbayes.score(x_test, y_test))\n",
    "print \"Linear SVC Score: {} \\n\".format(linearSVC.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the above results, this performed slightly better than the model generated with the sentiment score, positive/negative ratio and length of text. Multinomial Naive Bayes performed the best so far as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree score: 0.693131132917\n",
      "Random Forest score: 0.743086529884\n",
      "Multinomial Naive Bayes score: 0.817127564674 \n",
      "\n",
      "Decision Tree AUC: 0.674537784076\n",
      "Random Forest AUC: 0.766679499683\n",
      "Multinomial Naive Bayes AUC: 0.790659426578\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(text_array, good, test_size=0.3, random_state=12)\n",
    "tree = DecisionTreeClassifier()\n",
    "forest = RandomForestClassifier()\n",
    "mnbayes = MultinomialNB()\n",
    "\n",
    "tree.fit(x_train, y_train)\n",
    "forest.fit(x_train, y_train)\n",
    "mnbayes.fit(x_train, y_train)\n",
    "linearSVC.fit(x_train, y_train)\n",
    "\n",
    "print \"Decision Tree score: {}\".format(tree.score(x_test, y_test))\n",
    "print \"Random Forests score: {}\".format(forest.score(x_test, y_test))\n",
    "print \"Multinomial Naive Bayes score: {} \\n\".format(mnbayes.score(x_test, y_test))\n",
    "print \"Linear SVC Score: {} \\n\".format(linearSVC.score(x_test, y_test))\n",
    "\n",
    "print \"Decision Tree AUC: {}\".format(cross_val_score(tree, text_array, good, cv=3, scoring='roc_auc').mean())\n",
    "print \"Random Forests AUC: {}\".format(cross_val_score(forest, text_array, good, cv=3, scoring='roc_auc').mean())\n",
    "print \"Multinomial Naive Bayes AUC: {}\".format(cross_val_score(mnbayes, text_array, good, cv=3, scoring='roc_auc').mean())\n",
    "print \"Linear SVC AUC: {}\".format(cross_val_score(linearSVC, text_array, good, cv=3, scoring='roc_auc').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the vectorization of the words, we get a respectable AUC for MN Bayes when testing for good / bad reviews. Let's continue to try to improve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
